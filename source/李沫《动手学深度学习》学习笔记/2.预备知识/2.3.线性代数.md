---
layout: post
cover: /images/article/DeepLearning.jpg
---

# 2.3. 线性代数


## 2.3.1. 标量
* 仅包含一个数值的叫标量（scalar）（例如，$5$、$9$和$32$）
* 符号 x和 y 称为变量（variable），它们表示未知的标量值。
* 标量变量由普通小写字母表示（例如，$x$、$y$和$z$）。
* $\mathbb{R}$表示所有（连续）实数标量的空间（space）。
* 表达式$x\in\mathbb{R}$是表示$x$是一个实数标量的正式形式。
* $x, y \in \{0,1\}$来表明$x$和$y$是值只能为$0$或$1$的数字。

**标量用仅有一个元素的张量表示。**


```python
import torch
x=torch.tensor(4)
y=torch.tensor(5.0)

print(x+y);print(x-y);print(x*y);print(x/y);print(x**y);
```

    tensor(9.)
    tensor(-1.)
    tensor(20.)
    tensor(0.8000)
    tensor(1024.)
    

## 2.3.2. 向量
* 可认为向量是标量值组成的列表
* 这些标量值称为向量的元素（element）或分量（component）
* 在数学表示法中，通常将向量记为**粗体**、*小写*的符号 （例如，$\mathbf{x}$、$\mathbf{y}$和$\mathbf{z})$）。
* 使用下标来引用向量元素 （如$x_i$来引用向量$\mathbf{x}$中第$i$个元素）（元素$x_i$是一个标量，所以文字表示时不会加粗）
* 列向量是向量的默认方向,所以在数学中，向量$\mathbf{x}$一般写为
$$\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix}$$

**向量用一维张量表示**


```python
x = torch.arange(10)
x
```




    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])



**通过张量的索引来访问向量的元素**


```python
x[3]
```




    tensor(3)



## 2.3.2.1. 长度、维度和形状
* 向量的长度通常称为向量的维度（dimension）
* 由$n$个实值标量组成的向量$\mathbf{x}$,记作 $\mathbf{x} \in \mathbb{R}^n$

**通过python内置函数访问一维张量的长度**


```python
len(x)
```




    10



* **张量表示的是一个向量时 可用shape属性访问向量长度（元素个数）**
* **shape是一个数组，列出了张量每个轴的长度（元素个数）**
* **维度**
    + 向量(轴)的**维度**被用来表示向量(轴)的长度，即向量(轴)的元素数量。 
    + 张量的**维度**被用来表示张量的轴(向量)数，即张量的某个维度(轴,向量)的维数就是这个轴(向量)的长度
    


```python
x.shape
```




    torch.Size([10])



## 2.3.3. 矩阵
* 矩阵常用**粗体**、大写字母来表示 （例如，$\mathbf{X}$ $\mathbf{Y}$ $\mathbf{Z}$ ）
* 由$m$行$n$列实数值标量组成的矩阵$\mathbf{A}$，记作$ \mathbf{A} \in \mathbb{R}^{m \times n} $
* 相同数量行和列的矩阵称为方阵（square matrix）
* 矩阵在数学中一般写成一个行列式：
$$\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}$$
* 通过行索$i$列索引$j$引访问矩阵中的标量元素，写法为：$ [\mathbf{A}]_{ij} $
* 标量元素是一个未知数，可简写为：$ a_{ij} $
* 必要时才用逗号分隔行列索引，如：$ a_{2i+1,3j-1} $和$ [\mathbf{A}]_{2i+1,3j-1} $
* 矩阵的转置运算是指交换矩阵的行列元素($a_{ij}$ <==> $b_{ji}$)，符号为：$ \mathbf{A} ^ \top $
* 如果$ \mathbf{A} $ = $ \mathbf{B} ^ \top $ , 则$a_{ij}$ = $b_{ji}$
* 对称矩阵（symmetric matrix）： $ \mathbf{A} $=$ \mathbf{A} ^ \top $
* 单个向量的默认方向是列向量，但在表示表格数据集的矩阵中， 将每个数据样本作为矩阵中的行向量更为常见。(一行表示一个样本)


```python
A=torch.arange(16).reshape(4,4)
print(A)
print(A.T)
```

    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11],
            [12, 13, 14, 15]])
    tensor([[ 0,  4,  8, 12],
            [ 1,  5,  9, 13],
            [ 2,  6, 10, 14],
            [ 3,  7, 11, 15]])
    


```python
A=torch.tensor([
    [1,2,3],
    [2,0,4],
    [3,4,5]
])

A==A.T
```




    tensor([[True, True, True],
            [True, True, True],
            [True, True, True]])



## 2.3.4. 张量
* 张量用特殊字体的大写字母表示（例如，$\mathsf{X}$ $\mathsf{Y}$ $\mathsf{Z}$ ）
* 索引机制（例如$ x_{2i+1,3j-1} $和$ [\mathsf{X}]_{2i+1,3j-1,4k+1} $）与矩阵类似。


```python
X=torch.arange(24).reshape(2,3,4)
X
```




    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])



### 2.3.5. 张量算法的基本性质
* 按元素的一元运算不会改变其操作数的形状。 
* 按元素的二元运算的结果都将是相同形状的张量（给定形状相同两个张量）
    * 如：两个矩阵的按元素乘法称为Hadamard积（Hadamard product）[数学符号：$\odot$]
    * $\mathbf{A},\mathbf{B} \in \mathbb{R}^{m \times n}$，矩阵$\mathbf{A}$和$\mathbf{B}$的Hadamard积(哈达玛积)为：

$$
\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
\end{bmatrix}.
$$

**将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。**


```python
A=torch.arange(24).reshape(2,3,4)
B=torch.arange(24).reshape(2,3,4)

print(A*B)
print()
print()
print(A+5)


```

    tensor([[[  0,   1,   4,   9],
             [ 16,  25,  36,  49],
             [ 64,  81, 100, 121]],
    
            [[144, 169, 196, 225],
             [256, 289, 324, 361],
             [400, 441, 484, 529]]])
    
    
    tensor([[[ 5,  6,  7,  8],
             [ 9, 10, 11, 12],
             [13, 14, 15, 16]],
    
            [[17, 18, 19, 20],
             [21, 22, 23, 24],
             [25, 26, 27, 28]]])
    

### 对张量的求和（降维）
* 对张量求和是一个有用的运算
* 数学中，用$\sum$符号表示求和
* 向量$\mathbf{x}$（长度为$d$）中元素的总和，记为$\sum_{i=1}^dx_i$
* 矩阵$\mathbf{A}$（$m$行$n$列）中元素的总和，记为$\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$

**默认情况下（axis=None），求和函数会沿所有的轴降低张量的维度，使它变为一个标量。**



```python
# 对标量求和
x=torch.tensor(276)
print(x.sum())

# 对向量求和
x=torch.arange(24).reshape(24)
print(x.sum())

# 对矩阵求和
X=torch.arange(24).reshape(2,-1)
print(X.sum())

# 对三维张量求和
X=torch.arange(24).reshape(2,3,4)
print(X.sum())

```

    tensor(276)
    tensor(276)
    tensor(276)
    tensor(276)
    

**可以指定张量沿哪一个轴求和降低维度**

* 沿0轴求和axis=0
    * 对于标量，沿0轴求和的值为其各行元素之和，即本身
    * 对于向量，沿0轴求和的值为其各行元素之和，即各行元素之和
    * 对于矩阵，沿0轴求和的值为其各行元素之和，即各行向量之和（就是将各向量相加,结果仍为向量）
    * 对于3维张量，沿0轴求和的值为其各行元素之和，即各行矩阵之和 （就是将各矩阵相加,结果仍为矩阵）


```python
# 对标量沿0轴求和
x=torch.tensor(276)
print(x)
x2=x.sum(axis=0)
print(x2)
print(x.shape);print(x2.shape)
print("0轴消失")

# 对向量沿0轴求和
x=torch.arange(24).reshape(24)
print(x)
x2=x.sum(axis=0)
print(x2)
print(x.shape);print(x2.shape)
print("0轴消失")

# 对矩阵沿0轴求和
x=torch.arange(24).reshape(2,-1)
print(x)
x2=x.sum(axis=0)
print(x2)
print(x.shape);print(x2.shape)
print("0轴消失")

# 对三维张量沿0轴求和
x=torch.arange(24).reshape(2,3,4)
print(x)
x2=x.sum(axis=0)
print(x2)
print(x.shape);print(x2.shape)
print("0轴消失")
```

    tensor(276)
    tensor(276)
    torch.Size([])
    torch.Size([])
    0轴消失
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
            18, 19, 20, 21, 22, 23])
    tensor(276)
    torch.Size([24])
    torch.Size([])
    0轴消失
    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
            [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])
    tensor([12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34])
    torch.Size([2, 12])
    torch.Size([12])
    0轴消失
    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])
    tensor([[12, 14, 16, 18],
            [20, 22, 24, 26],
            [28, 30, 32, 34]])
    torch.Size([2, 3, 4])
    torch.Size([3, 4])
    0轴消失
    

* 沿1轴求和axis=1
    * 标量只有0轴
    * 向量只有0轴
    * 对于矩阵，沿1轴求和的值为各列元素之和，即为列向量之和（就是将各列各向量相加,结果仍为向量）
    * 对于3维张量，沿1轴求和的值为各列元素之和，即为列矩阵之和 （就是将各列各矩阵相加,结果仍为矩阵）


```python
# 对矩阵沿1轴求和
x=torch.arange(24).reshape(2,-1)
print(x)
x2=x.sum(axis=1)
print(x2)
print(x.shape);print(x2.shape)
print("1轴消失")


# 对三维张量沿1轴求和
x=torch.arange(24).reshape(2,3,4)
print(x)
x2=x.sum(axis=1)
print(x2)
print(x.shape);print(x2.shape)
print("1轴消失")

```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],
            [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])
    tensor([ 66, 210])
    torch.Size([2, 12])
    torch.Size([2])
    1轴消失
    tensor([[[ 0,  1,  2,  3],
             [ 4,  5,  6,  7],
             [ 8,  9, 10, 11]],
    
            [[12, 13, 14, 15],
             [16, 17, 18, 19],
             [20, 21, 22, 23]]])
    tensor([[12, 15, 18, 21],
            [48, 51, 54, 57]])
    torch.Size([2, 3, 4])
    torch.Size([2, 4])
    1轴消失
    

**沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。**


```python
# 对矩阵沿0、1轴求和
x=torch.arange(24).reshape(2,-1)

x.sum(axis=[0, 1])  # x.sum()
```




    tensor(276)



### 张量的平均值(mean或average)
* 总和sum/元素总数numel=平均值mean
* 在代码中，我们可以调用函数来计算任意形状张量的平均值。


```python
X=torch.arange(24.0).reshape(2,12)

print(    X.mean()   )

print(    X.sum() / X.numel()         )
```

    tensor(11.5000)
    tensor(11.5000)
    

**沿指定轴降低张量的维度计算平均值**


```python
X=torch.arange(24.0).reshape(2,12)

# print(    X.shape   )
# print(    X.shape[0]   )

print(    X.mean(axis=0)   )

print(    X.sum(axis=0) / X.shape[0]         )
```

    tensor([ 6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17.])
    tensor([ 6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17.])
    

### 非降维求和
* 计算总和或均值时保持轴数不变`keepdim=True`


```python
X=torch.arange(36.0).reshape(3,3,4)
print(X)

print("非降维求和:")
print(X.sum(axis=1,keepdim=True))

print("降维求和:")
print(X.sum(axis=1,keepdim=False))
```

    tensor([[[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.]],
    
            [[12., 13., 14., 15.],
             [16., 17., 18., 19.],
             [20., 21., 22., 23.]],
    
            [[24., 25., 26., 27.],
             [28., 29., 30., 31.],
             [32., 33., 34., 35.]]])
    非降维求和:
    tensor([[[12., 15., 18., 21.]],
    
            [[48., 51., 54., 57.]],
    
            [[84., 87., 90., 93.]]])
    降维求和:
    tensor([[12., 15., 18., 21.],
            [48., 51., 54., 57.],
            [84., 87., 90., 93.]])
    

**有了非降维的矩阵，再结合广播机制，就可以按元素计算 `x:x所在列的平均值`。**


```python
X=torch.arange(36.0).reshape(3,3,4)
X_mean=X.mean(axis=1,keepdim=True)

print(X)
print(X_mean)
X/X_mean
```

    tensor([[[ 0.,  1.,  2.,  3.],
             [ 4.,  5.,  6.,  7.],
             [ 8.,  9., 10., 11.]],
    
            [[12., 13., 14., 15.],
             [16., 17., 18., 19.],
             [20., 21., 22., 23.]],
    
            [[24., 25., 26., 27.],
             [28., 29., 30., 31.],
             [32., 33., 34., 35.]]])
    tensor([[[ 4.,  5.,  6.,  7.]],
    
            [[16., 17., 18., 19.]],
    
            [[28., 29., 30., 31.]]])
    




    tensor([[[0.0000, 0.2000, 0.3333, 0.4286],
             [1.0000, 1.0000, 1.0000, 1.0000],
             [2.0000, 1.8000, 1.6667, 1.5714]],
    
            [[0.7500, 0.7647, 0.7778, 0.7895],
             [1.0000, 1.0000, 1.0000, 1.0000],
             [1.2500, 1.2353, 1.2222, 1.2105]],
    
            [[0.8571, 0.8621, 0.8667, 0.8710],
             [1.0000, 1.0000, 1.0000, 1.0000],
             [1.1429, 1.1379, 1.1333, 1.1290]]])



## 向量的点积（Dot Product）

* 给定两个向量$\mathbf{x},\mathbf{y}\in\mathbb{R}^n$，
$$\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},

\mathbf{y} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix}。$$

* 它们的*点积*（dot product）(写作：$\mathbf{x} \cdot \mathbf{y}$ 或 $\mathbf{x}^\top\mathbf{y}$ 或 $\langle\mathbf{x},\mathbf{y}\rangle$)
* 是相同位置的按元素乘积的和：$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$。
* 点积为0的两向量互相垂直(正交)
    + 如果$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i = 0$，则$\mathbf{x} \perp \mathbf{y}$。


**点积是两向量中相同位置的按元素乘积的和**
* 使用函数计算点积
    + torch.dot(x,y)
* 根据点积定义,可通过按元素乘法然后进行求和计算点积
    + torch.sum(x*y)
* 关于点积的定义为什么可以写成：$\mathbf{x} \cdot \mathbf{y} = \mathbf{x}^\top \times \mathbf{y}$
    * 首先，线性代数中，T表示转置，列向量的转置是一个行向量
    * 其次，线性代数中，n行m列的矩阵A与i行j列的矩阵B相乘 结果是一个n行j列的矩阵C
    * 再者，线性代数中，矩阵C中c_ij=求和(a_ij * b_ij)
    * 由此，(x^T) X y 就是一个行的矩阵乘以一个一列的矩阵，结果就是一个标量，就是点积的值
    


```python
x = torch.arange(12).reshape(12)
y = torch.arange(12).reshape(12)

torch.dot(x,y),torch.sum(x*y)
```




    (tensor(506), tensor(506))



## 矩阵-向量积

* *矩阵-向量积*（matrix-vector product）。
* 对于矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和向量$\mathbf{x} \in \mathbb{R}^n$：

$$
\mathbf{A}=
\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}，
\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix}。
$$

* 可将矩阵$\mathbf{A}$用它的行向量表示（其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行。）：

$$\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},$$

* 于是矩阵向量积$\mathbf{Ax}$就可以写成
$$
\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}

\begin{bmatrix}
\mathbf{x}_{1} \\
\mathbf{x}_{2} \\
\vdots \\
\mathbf{x}_n \\
\end{bmatrix}

= 

\begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.
$$

* 由此，`矩阵-向量积`就转换成了求向量的各个元素，求各个元素就是求`向量点积（向量-向量）`

* 矩阵-向量积，使用`mv`函数。
* 如`torch.mv(A, x)`其中A为矩阵，x为向量
* 注意，`A`的列数（沿轴1的长度）必须与`x`的维数（其长度）相同。



```python
A = torch.arange(12).reshape(3,4)
x = torch.arange(4)

torch.mv(A,x)
```




    tensor([14, 38, 62])



## 矩阵-矩阵乘法

* **矩阵-矩阵乘法**(matrix-matrix multiplication)
* 设有两个矩阵,矩阵$\mathbf{A} \in \mathbb{R}^{n \times k}$和矩阵$\mathbf{B} \in \mathbb{R}^{k \times m}$：

$$\mathbf{A}=\begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1k} \\
 a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nk} \\
\end{bmatrix},\quad
\mathbf{B}=\begin{bmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
 b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{bmatrix}.$$

* 矩阵$\mathbf{A}$用行向量$\mathbf{a}^\top_{i} \in \mathbb{R}^k$表示。
* 矩阵$\mathbf{B}$用列向量$\mathbf{b}_{j} \in \mathbb{R}^k$表示。

$$
\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix},
\quad \mathbf{B}=\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}.
$$

* 此时矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$,就可以表示为：

$$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.
$$

* 由此，`矩阵-矩阵的乘法`就转换成了求矩阵的各个元素，求各个元素就是求`向量点积（向量-向量）`

* 矩阵-矩阵乘法，使用`mm`函数。
* 如`torch.mm(A, B)`其中A、B为矩阵
* AB=C中，C行数是A的行数，C的列数是B的列数


```python
A = torch.arange(12).reshape(3,4)
B = torch.arange(12).reshape(4,3)

torch.mm(A,B)
```




    tensor([[ 42,  48,  54],
            [114, 136, 158],
            [186, 224, 262]])



## 向量的范数
* 向量的范数的概念是向量的模的概念的推广
* 范数描述了一个向量的大小
* 范数是一个函数
* 向量的范数的定义：
    + 对于任意有个$n$个元素的向量$\mathbf{x} \in \mathbb{R}^n$，
    + 其范数$L_p$为：
    $$
        \|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
        (p \geq 0)
    $$
* 向量的范数的三个性质：
    1. 非负性
        + $$
                f(\mathbf{x}) \geq 0.
        $$
    2. 正齐次性
        + $$
            f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).
        $$
    3. 三角不等式（不完全可加性）
        + $$
            f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).
        $$
* 向量的模就是欧几里得范数，写做$L_2$：
    + $$
        \|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
    $$
* $L_2$范数常省略下标$2$（$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$）


**向量$\mathbf{x}$的$L_2$范数可通过函数`torch.norm(x)`计算**


```python
x=torch.arange(12.0)

torch.norm(x)
```




    tensor(22.4944)



* 在深度学习中，更常使用的范数是$(L_2)^2$ 和 $L_1$
    * $L_1$范数，它表示为向量元素的绝对值之和.
        + $$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$$
    * $(L_2)^2$范数的平方.
        + $$(\|\mathbf{x}\|_1)^2 = \sum_{i=1}^n \left|x_i \right|^2.$$
* 与$L_2$范数相比，$L_1$范数受异常值的影响较小。
* 根据定义，计算向量$\mathbf{x}$的$L_1$范数，先对其按元素求绝对值，再求其和。
    + torch.abs(x).sum()



```python
x=torch.arange(12.0)

torch.abs(x).sum()
```




    tensor(66.)



## 矩阵的范数
* 矩阵范数不存在公认唯一的度量方式。
* Frobenius 范数（F-范数）（Frobenius norm）
    + Frobenius范数满足向量范数的所有性质，类似于向量的$L_2$范数。
    + 矩阵$\mathbf{X} \in \mathbb{R}^{m \times n}$的Frobenius范数是矩阵各元素的平方的和的平方根：
    + $$
        \|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.
        $$

**矩阵$\mathbf{X}$的Frobenius范数可通过函数`torch.norm(X)`计算**


```python
x=torch.arange(12.0).reshape(3,4)

torch.norm(x)
```




    tensor(22.4944)



### 范数和目标
在深度学习中，经常试图解决优化问题：
* *最大化*分配给观测数据的概率;
* *最小化*预测和真实观测之间的距离。
* 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。
* 目标，通常被表达为范数。

## 关于线性代数的更多信息
[线性代数运算的在线附录](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)

## 小结

* 标量、向量、矩阵和张量是线性代数中的基本数学对象。
* 向量泛化自标量，矩阵泛化自向量。
* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。
* 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。
* 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。
* 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。
* 我们可以对标量、向量、矩阵和张量执行各种操作。

## 练习

1. 证明一个矩阵$\mathbf{A}$的转置的转置是$\mathbf{A}$，即$(\mathbf{A}^\top)^\top = \mathbf{A}$。
    * 显而易见
    
1. 给出两个矩阵$\mathbf{A}$和$\mathbf{B}$，证明“它们转置的和”等于“它们和的转置”，即$\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$。
    + 显而易见
    
1. 给定任意方阵$\mathbf{A}$，$\mathbf{A} + \mathbf{A}^\top$总是对称的吗?为什么?
    + ？？？
    
1. 我们在本节中定义了形状$(2,3,4)$的张量`X`。`len(X)`的输出结果是什么？
    + 当给定一个2x3x4的三维张量时，这个张量其实是一个拥有2个元素个向量，其中每个元素都是一个3x4的矩阵
    + len()计算的是给定向量的元素个数，所以当给的是一个当给定一个2x3x4的三维张量时，len()的值为2
    
1. 对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?
    + 是0轴的长度，原因同上
    
1. 运行`A/A.sum(axis=1)`，看看会发生什么。你能分析原因吗？
    + 会报错，A.sum(axis=1)会将张量A由2x3x4的形状变成2x4,按元素运算的两张量必须形状相同，或者不相同的边是只有一个维度的（如3x4的矩阵能与3x1的矩阵按元素运算也能1x4的矩阵按元素运算（广播机制））

1. 考虑一个具有形状$(2,3,4)$的张量，在轴0、1、2上的求和输出是什么形状?
    + 输出应该是一个标量
    
1. 为`torch.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?
    + 无论多少维的张量，torch.norm()函数计算的都是所有元素的平方和



```python
# 第四题
X=torch.arange(24).reshape(2,3,4)
len(X)
```




    2




```python
# 第六题
X=torch.arange(24).reshape(2,3,4)
X/X.sum(axis=1)
```


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    C:\Users\ADMINI~1\AppData\Local\Temp/ipykernel_8836/843161202.py in <module>
          1 X=torch.arange(24).reshape(2,3,4)
    ----> 2 X/X.sum(axis=1)
    

    RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1



```python
# 第七题
X=torch.arange(24).reshape(2,3,4)
X.sum(axis=[0,1,2])
```




    tensor(276)




```python
# 第八题
X=torch.arange(24.0*2*3*4*5)
print(torch.norm(X))

X.reshape(2,-1)
print(torch.norm(X))

X.reshape(3,2,-1)
print(torch.norm(X))

X.reshape(4,3,2,-1)
print(torch.norm(X))

X.reshape(5,4,3,2,-1)
print(torch.norm(X))
```

    tensor(89210.3281)
    tensor(89210.3281)
    tensor(89210.3281)
    tensor(89210.3281)
    tensor(89210.3281)
    
