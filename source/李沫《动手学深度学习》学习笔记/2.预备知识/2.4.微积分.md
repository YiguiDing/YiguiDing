---
title: 2.4.微积分
layout: post
cover: /images/article/DeepLearning.jpg
---

# 微积分
* 逼近法就是积分（intergral calculus）的起源
* 微分（differential calculus）最重要的应用是优化问题。

## 导数和微分
### 可导，导数，导函数
* 假设有一个函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。
    * 在某点可导（存在导数）的条件
        + 函数在该点的去心领域内有定义
        + 函数在该点存在左导数和右导数
        + 左导数=右导数
    * 在某点的导数
        * 如果函数$f(x)$在$x=x_0$处存在导数，其在该点的导数定义为：
            + $$
                f'(x_0) = \lim_{h \rightarrow 0} \frac{f(x_0+h) - f(x_0)}{h}.
            $$
    * 导函数
        * 如果函数$f(x)$在x的定义域内处处可导，其导函数的定义为：
            + $$
                f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.
            $$

### 可微，微分
* 在某点可微（differentiable）
    + 对于一元函数，在某点可导<--->在该点可微
    + 对于多元函数，（在某点可偏导+偏导连续）<---> 在该点可微
* 在区间可微
    + 如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。
* 在某点的微分
    + 对于一元函数
        + 如果$f'(a)$存在（可导）或者说在$f(x),x=a$处可微，其在该点的微分为$Df(a)=f'(a)\times dx$
    + 对于多元函数
        + ？？？
* 在区间的微分
    + 对于一元函数
        + 如果$f(x)$在x的定义域内处处可导，或者说处处可微，其在该点的微分为$Df(x)=f'(x)\times dx$
    + 对于多元函数
        + ？？？？



**实验解释导数**
* 定义$u'=f(x)=3x^2-4x$
* 令$x=1$并让$h$接近$0$
* $\frac{f(x+h)-f(x)}{h}$的数值结果将接近$2$
* 稍后会看到，导数$f'(1)$是$2$。



```python
def f(x):
    return 3 * x ** 2 - 4 * x

def numerical_lim(f, x, h):#求微分
    return (f(x + h) - f(x)) / h

h = 0.1
for i in range(5):
    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
    h *= 0.1
```

    h=0.10000, numerical limit=2.30000
    h=0.01000, numerical limit=2.03000
    h=0.00100, numerical limit=2.00300
    h=0.00010, numerical limit=2.00030
    h=0.00001, numerical limit=2.00003
    

### 导数和微分的几个等价符号
给定$y=f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量。以下表达式是等价的：

$$f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),$$

其中符号$\frac{d}{dx}$和$D$是*微分运算符*，表示*微分*操作。
我们可以使用以下规则来对常见函数求微分：

* $DC = 0$（$C$是一个常数）
* $Dx^n = nx^{n-1}$（*幂律*（power rule），$n$是任意实数）
* $De^x = e^x$
* $D\ln(x) = 1/x$

为了微分一个由一些常见函数组成的函数，下面的一些法则方便使用。
假设函数$f$和$g$都是可微的，$C$是一个常数，则：

*常数相乘法则*
$$\frac{d}{dx} [Cf(x)] = C \frac{d}{dx} f(x),$$

*加法法则*

$$\frac{d}{dx} [f(x) + g(x)] = \frac{d}{dx} f(x) + \frac{d}{dx} g(x),$$

*乘法法则*

$$\frac{d}{dx} [f(x)g(x)] = f(x) \frac{d}{dx} [g(x)] + g(x) \frac{d}{dx} [f(x)],$$

*除法法则*

$$\frac{d}{dx} \left[\frac{f(x)}{g(x)}\right] = \frac{g(x) \frac{d}{dx} [f(x)] - f(x) \frac{d}{dx} [g(x)]}{[g(x)]^2}.$$


* 现在我们可以应用上述几个法则来计算$u'=f'(x)=3\frac{d}{dx}x^2-4\frac{d}{dx}x=6x-4$。
* 令$x=1$，我们有$u'=2$：在这个实验中，数值结果接近$2$，
* 这一点得到了我们在本节前面的实验的支持。
* 当$x=1$时，此导数也是曲线$u=f(x)$切线的斜率。

* [**为了对导数的这种解释进行可视化，我们将使用`matplotlib`**]，
* 这是一个Python中流行的绘图库。
* 要配置`matplotlib`生成图形的属性，我们需要(**定义几个函数**)。
* 在下面，`use_svg_display`函数指定`matplotlib`软件包输出svg图表以获得更清晰的图像。
* 
* 注意，注释`#@save`是一个特殊的标记，会将对应的函数、类或语句保存在`d2l`包中。
* 因此，以后无须重新定义就可以直接调用它们（例如，`d2l.use_svg_display()`）。

**导入库文件**


```python
#!pip install IPython
# from IPython import display

%matplotlib inline
# !pip install matplotlib
# import matplotlib

import matplotlib_inline
import matplotlib
import numpy
```

**定义`use_svg_display`函数使得可以用svg格式在jupyter中绘图**


```python
def use_svg_display():  #@save
    """使用svg格式在Jupyter中显示绘图"""
    #display.set_matplotlib_formats('svg')
    matplotlib_inline.backend_inline.set_matplotlib_formats('svg')
```

**定义`set_figsize`函数来设置图表大小。**


```python
def set_figsize(figsize=(3.5, 2.5)):  #@save
    """设置matplotlib的图表大小"""
    use_svg_display()
    matplotlib.pyplot.rcParams['figure.figsize'] = figsize
```

**`set_axes`函数用于设置由`matplotlib`生成图表的轴的属性。**


```python
#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """设置matplotlib的轴"""
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()
```

**通过以上三个用于图形配置的函数，可定义`plot`函数来简洁地绘制多条曲线**


```python
#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """绘制数据点"""
    if legend is None:
        legend = []

    set_figsize(figsize)
    # axes = axes if axes else d2l.plt.gca()
    axes = axes if axes else matplotlib.pyplot.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)

```

现在我们可以[**绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$**]，
其中系数$2$是切线的斜率。



```python
def f(x):
    return 3 * x ** 2 - 4 * x

x = numpy.arange(0, 3, 0.1)


plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
```


    
![svg](/李沫《动手学深度学习》学习笔记/2.预备知识/output_18_0.svg)
    


## 偏导数
* 偏导数是将微分的概念在多元函数（multivariate function）上的推广
* 设$y = f(x_1, x_2, \ldots, x_n)$是一个具有$n$个变量的函数。
* 计算$\frac{\partial y}{\partial x_i}$，
只需将将$x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n$看作常数，
并计算$y$关于$x_i$的导数。
* $y$关于第$i$个参数$x_i$的*偏导数*（partial derivative）为：


$$ \frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.$$


* 对于偏导数的表示，以下是等价的：

$$\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$


## 梯度

* 我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。
* 具体而言，设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是
* 一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，并且输出是一个标量。
* 函数$f(\mathbf{x})$相对于$\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:

$$\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,$$

* 其中$\nabla_{\mathbf{x}} f(\mathbf{x})$通常在没有歧义时被$\nabla f(\mathbf{x})$取代。

假设$\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:

* 对于所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$
* 对于所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A}  = \mathbf{A}$
* 对于所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x}  = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$
* $\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$

同样，对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}$。
正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。


## 链式法则

然而，上面方法可能很难找到梯度。
这是因为在深度学习中，多元函数通常是*复合*（composite）的，
所以我们可能没法应用上述任何规则来微分这些函数。
幸运的是，链式法则使我们能够微分复合函数。

让我们先考虑单变量函数。假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：

$$\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}.$$

现在让我们把注意力转向一个更一般的场景，即函数具有任意数量的变量的情况。
假设可微分函数$y$有变量$u_1, u_2, \ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \ldots, x_n$。
注意，$y$是$x_1, x_2， \ldots, x_n$的函数。
对于任意$i = 1, 2, \ldots, n$，链式法则给出：

$$\frac{dy}{dx_i} = \frac{dy}{du_1} \frac{du_1}{dx_i} + \frac{dy}{du_2} \frac{du_2}{dx_i} + \cdots + \frac{dy}{du_m} \frac{du_m}{dx_i}$$



## 小结

* 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。
* 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。
* 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。
* 链式法则使我们能够微分复合函数。

## 练习

1. 绘制函数$y = f(x) = x^3 - \frac{1}{x}$和其在$x = 1$处切线的图像。
1. 求函数$f(\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。
1. 函数$f(\mathbf{x}) = \|\mathbf{x}\|_2$的梯度是什么？
1. 你可以写出函数$u = f(x, y, z)$，其中$x = x(a, b)$，$y = y(a, b)$，$z = z(a, b)$的链式法则吗?

## 作答
### 题1 绘制函数$y = f(x) = x^3 - \frac{1}{x}$和其在$x = 1$处切线的图像。


```python
def f(x):
    #x^3-x^(-1)的导函数为3x^2+1/x^2
    # k=f'(1)=3*1^2+1/1^2=3+1=4
    # 当x=1时，y=f(x)=1^3-1/1=0
    # 所以设y=kx+b得0=4*1+b,得b=-4
    # 所以其在x=1处切线的表达式为y=4x-4
    return x*x*x-1/x

x = numpy.arange(0.1, 2, 0.1)

plot(x, [f(x), 4*x-4  ], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
```


    
![svg](/李沫《动手学深度学习》学习笔记/2.预备知识/output_24_0.svg)
    


### 题2 求函数$f(\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。
#### 答：其梯度为：


$$
\nabla_{\mathbf{x}} f(\mathbf{x})
= 
 \bigg[\frac{\partial (3x_1^2 + 5e^{x_2}) }{\partial x_1}, \frac{\partial (3x_1^2 + 5e^{x_2}) }{\partial x_2}\bigg]^\top 
= \bigg[6x_1, 5e^{x_2}\bigg]^\top 
$$

### 题3 函数$f(\mathbf{x}) = \|\mathbf{x}\|_2$的梯度是什么？

#### 答

$$
f(\mathbf{x})
=
\|\mathbf{x}\|_2
=
\sqrt{\sum_{i=1}^n x_i^2}
$$

$$
\begin{equation}
    \begin{aligned}
        \nabla_{\mathbf{x}} f(\mathbf{x}) 
        = \nabla_{\mathbf{x}} \| \mathbf{x} \| _2
        &=\bigg[
                \frac{\partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) }{\partial x_1},
                \frac{\partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) }{\partial x_2},
                \dots ,
                \frac{\partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) }{\partial x_n}    
        \bigg]^\top \\
        &=\bigg[
                \frac{ \partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) } { \partial \sum_{i=1}^n x_i^2}  \times  \frac{\partial (   \sum_{i=1}^n x_i^2   ) } {\partial x_1},
                \frac{ \partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) } { \partial \sum_{i=1}^n x_i^2}  \times  \frac{\partial (   \sum_{i=1}^n x_i^2   ) } {\partial x_2},
                \dots ,
                \frac{ \partial (   \sqrt{\sum_{i=1}^n x_i^2}   ) } { \partial \sum_{i=1}^n x_i^2}  \times  \frac{\partial (   \sum_{i=1}^n x_i^2   ) } {\partial x_n}
        \bigg]^\top \\
        &=\bigg[
                \frac{1}{2} ({\sum_{i=1}^n x_i^2})^{-\frac{1}{2}} \times 2x_1^2,
                \frac{1}{2} ({\sum_{i=1}^n x_i^2})^{-\frac{1}{2}} \times 2x_2^2,
                \dots ,
                \frac{1}{2} ({\sum_{i=1}^n x_i^2})^{-\frac{1}{2}} \times 2x_n^2
        \bigg]^\top \\
        &=\bigg[
                \frac{1}{2} \frac{1}{ \sqrt{\sum_{i=1}^n x_i^2} } \times 2x_1^2,
                \frac{1}{2} \frac{1}{ \sqrt{\sum_{i=1}^n x_i^2} } \times 2x_2^2,
                \dots ,
                \frac{1}{2} \frac{1}{ \sqrt{\sum_{i=1}^n x_i^2} } \times 2x_n^2
        \bigg]^\top \\
        &=\bigg[
                \frac{ x_1^2 }{ \sqrt{\sum_{i=1}^n x_i^2} },
                \frac{ x_2^2 }{ \sqrt{\sum_{i=1}^n x_i^2} },
                \dots ,
                \frac{ x_n^2 }{ \sqrt{\sum_{i=1}^n x_i^2} }
        \bigg]^\top \\
        &=\bigg[
                \frac{ x_1^2 }{ \| \mathbf{x} \| _2 },
                \frac{ x_2^2 }{ \| \mathbf{x} \| _2 },
                \dots ,
                \frac{ x_n^2 }{ \| \mathbf{x} \| _2 }
        \bigg]^\top \\
        &=\begin{bmatrix}
                \frac{ x_1^2 }{ \| \mathbf{x} \| _2 }   \\
                \frac{ x_2^2 }{ \| \mathbf{x} \| _2 }   \\
                \vdots                                  \\
                \frac{ x_n^2 }{ \| \mathbf{x} \| _2 }   \\
        \end{bmatrix} \\
        &=\frac{ \mathbf{x} }{ \| \mathbf{x} \| _2 } \\
    \end{aligned}
\end{equation}
$$

