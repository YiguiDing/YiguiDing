---
title: 3.5图像分类数据集合
layout: post
cover: /《动手学深度学习》(李沫)学习笔记/cover.jpg
---

# 图像分类数据集

* **MNIST数据集**
    * 图像分类中广泛使用的数据集之一
* **Fashion-MNIST**
    * 类似但更复杂的数据集


```python
import torch
import torch.utils.data as data #用于构造打包随机抽取小批数据的函数
import torchvision #专门用来处理图像的库
# import torchvision.datasets as datasets #用于加载现成数据集
# import torchvision.transforms as transforms #用于对图像数据操作
import matplotlib.pyplot as pyplot #matplotlib绘图库
```

* 使用torch框架内置函数下载fashion-mnist数据集的训练集和测试集合


```python
fashion_train = torchvision.datasets.FashionMNIST(
                                                    root="./fasionMNIST",
                                                    train=True,
                                                    transform=torchvision.transforms.ToTensor(),#否则返回的是一个图片
                                                    download=True
                                                    )
fashion_test = torchvision.datasets.FashionMNIST(
                                                    root="./fasionMNIST",
                                                    train=False,
                                                    transform=torchvision.transforms.ToTensor(),
                                                    download=True
                                                    )

print(fashion_train.data.shape)
print(fashion_train.targets.shape)

for k in range(4,5):#显示测试
    for i in range(28):
        for j in range(28):
            if(fashion_train.data[k,i,j]>1):
                print("+",end="  ")
            else:
                print(" ",end="  ")    
        print("")
    print(fashion_train.classes[fashion_train.targets[k]])
    break
```

    torch.Size([60000, 28, 28])
    torch.Size([60000])
                                                       +                                
                            +  +  +  +           +  +  +  +  +                          
                            +  +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +     +  +  +  +  +                             
                            +  +  +  +  +     +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                               +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +                             
                            +  +  +  +  +  +  +  +  +  +  +  +                          
                            +  +  +  +  +  +  +  +  +  +  +  +                          
                            +  +  +  +  +  +  +  +  +  +  +  +                          
                            +  +  +  +  +  +  +  +  +  +  +  +  +                       
                         +  +  +  +  +  +  +  +  +  +  +  +  +  +                       
                         +  +  +  +  +  +  +  +  +  +  +  +  +  +                       
                         +  +  +  +  +  +  +  +  +  +  +  +  +  +                       
                         +  +  +  +  +  +  +     +  +  +  +  +  +                       
                         +  +  +  +  +  +  +     +  +  +  +  +  +                       
                            +  +  +  +  +  +     +  +  +  +  +  +                       
                            +  +  +  +  +  +     +  +  +  +  +                          
                                  +  +  +  +     +  +  +                                
    T-shirt/top
    


```python
# TensorDataset = data.TensorDataset(fashion_train.data,fashion_train.targets)
# DataLoader = data.DataLoader(dataset=TensorDataset,batch_size=10,shuffle=True)

def DataLoader(Data,batch_size,disOrder=True):
    TensorDataset = data.TensorDataset(*Data)
    DataLoader = data.DataLoader(dataset=TensorDataset,batch_size=batch_size,shuffle=disOrder)
    return DataLoader

inputs_num = imgSize = 28*28
outputs_num = 10

W = torch.normal(0,0.01,size=(inputs_num,outputs_num),dtype=torch.double,requires_grad=True)
B = torch.normal(0,0.01,size=(1,outputs_num),dtype=torch.double,requires_grad=True)

def net(X):
    X = X.reshape(-1,imgSize).to(torch.double)
    Y_temp = torch.matmul(X,W)+B
    Y = softMax(Y_temp)
    return Y

def softMax(Y_temp):#Y_temp必须是一个二维向量，（1行，n列）也是二维向量
    exp = torch.exp(Y_temp) # 所有元素做e的指数运算
    list_sum = torch.sum(exp,dim=1,keepdim=True) # 按行求和 保持维度
    p = exp/list_sum # 按元素除法
    # print(torch.sum(p,dim=1,keepdim=True))#按行求和为1即概率和为1 说明计算正确
    # return p.float()
    return p

def error_Count(Y,Y_hat):
    count_Error = 0
    Y_hat_maxIndex = getMaxIndex(Y_hat)
    for i in range(len(Y_hat)):
        if( Y_hat_maxIndex[i]!=Y[i] ):
            count_Error+=1
    return count_Error
    

def getMaxIndex(Y_hat):
    outPutList = torch.arange(len(Y_hat))
    for i in range(len(Y_hat)):
        maxIndex = 0
        for j in range(outputs_num):
            if(Y_hat[i,j]>Y_hat[i,maxIndex]):
                maxIndex=j
        outPutList[i] = maxIndex
    return outPutList
    
def MSELoss(Y_hat,Y):
    Y_temp = torch.zeros_like(Y_hat)
    Y_temp[range(len(Y_hat)),Y] = 1.0
    return 1/2*(Y_temp-Y_hat)**2

def cross_LossV1(Y_hat,Y):
    Y_temp = torch.zeros_like(Y_hat)
    Y_temp[range(len(Y_hat)),Y] = 1.0
    return -torch.sum(torch.log(Y_hat)*Y_temp,dim=1,keepdim=True)

def cross_LossV2(Y_hat,Y):
    return -torch.log(  Y_hat[ range(len(Y_hat)) ,Y ]   ) 

def Sgd(args,learningRate,batch_size):
    with torch.no_grad():
        for arg in args:
            arg -= learningRate * arg.grad / batch_size
            arg.grad.zero_()
```


```python
NET=net
LOSS = cross_LossV2
SGD = Sgd

epoch = 5
batch_size = 50
learningRate = 0.000001

for epochTimes in range(epoch):
    iterableBatchData = DataLoader((fashion_train.data,fashion_train.targets),batch_size,True) #打乱小批量数据
    for imgs,labs in iterableBatchData:

        l = LOSS(NET(imgs),labs)
        
        l.sum().backward()
        SGD((W,B),learningRate,batch_size)

        # print("模型输出:\n",NET(imgs))
        # print("损失:",l)
        # print("预测值:",getMaxIndex(NET(imgs),batch_size))
        # print("真实值:",labs)
        # break
    print("训练第",epochTimes+1,"次后:")

    y_hat = NET(fashion_train.data)
    y_real = fashion_train.targets
    print("\t在训练数据集上的损失为:",end="")
    print(LOSS(y_hat,y_real).sum()/len(fashion_train),end="")
    # print("\t在训练数据集上的错误率为:",error_Count(y_real,y_hat),":",len(fashion_train),end="")
    print()

    y_hat = NET(fashion_test.data)
    y_real = fashion_test.targets
    print("\t在测试数据集上的损失为:",end="")
    print(LOSS(y_hat,y_real).sum()/len(fashion_test),end="")
    # print("\t在测试数据集上的错误率为:",error_Count(y_real,y_hat),":",len(fashion_test),end="")
    print()

```

    训练第 1 次后:
    	在训练数据集上的损失为:tensor(4.7130, dtype=torch.float64, grad_fn=<DivBackward0>)
    	在测试数据集上的损失为:tensor(4.9036, dtype=torch.float64, grad_fn=<DivBackward0>)
    训练第 2 次后:
    	在训练数据集上的损失为:tensor(3.6578, dtype=torch.float64, grad_fn=<DivBackward0>)
    	在测试数据集上的损失为:tensor(3.8411, dtype=torch.float64, grad_fn=<DivBackward0>)
    训练第 3 次后:
    	在训练数据集上的损失为:tensor(3.1790, dtype=torch.float64, grad_fn=<DivBackward0>)
    	在测试数据集上的损失为:tensor(3.3740, dtype=torch.float64, grad_fn=<DivBackward0>)
    训练第 4 次后:
    	在训练数据集上的损失为:tensor(2.8935, dtype=torch.float64, grad_fn=<DivBackward0>)
    	在测试数据集上的损失为:tensor(3.0951, dtype=torch.float64, grad_fn=<DivBackward0>)
    训练第 5 次后:
    	在训练数据集上的损失为:tensor(2.7048, dtype=torch.float64, grad_fn=<DivBackward0>)
    	在测试数据集上的损失为:tensor(2.8886, dtype=torch.float64, grad_fn=<DivBackward0>)
    


```python
# 测试
epoch = 1
batch_size = 1
for epochTimes in range(epoch):
    iterableBatchData = DataLoader((fashion_test.data,fashion_test.targets),batch_size,True) #打乱小批量数据
    for imgs,labs in iterableBatchData:
        break
    
    print("训练第",epochTimes+1,"次后:")

    y_hat = NET(fashion_train.data)
    y_real = fashion_train.targets
    print("\t在训练数据集上的损失为:",end="")
    print(LOSS(y_hat,y_real).sum()/len(fashion_train),end="")
    print("\t在训练数据集上的错误率为:",error_Count(y_real,y_hat),":",len(fashion_train),end="")
    print()

    y_hat = NET(fashion_test.data)
    y_real = fashion_test.targets
    print("\t在测试数据集上的损失为:",end="")
    print(LOSS(y_hat,y_real).sum()/len(fashion_test),end="")
    print("\t在测试数据集上的错误率为:",error_Count(y_real,y_hat),":",len(fashion_test),end="")    
```

    训练第 1 次后:
    	在训练数据集上的损失为:tensor(2.7048, dtype=torch.float64, grad_fn=<DivBackward0>)	在训练数据集上的错误率为: 16688 : 60000
    	在测试数据集上的损失为:tensor(2.8886, dtype=torch.float64, grad_fn=<DivBackward0>)	在测试数据集上的错误率为: 2918 : 10000

## 使用GPU计算



```python
import torch
import torchvision
# 1.查看当前pytorch是否为支持GPU版本
print(torch.__version__)
print(torchvision.__version__)
# 输出为：
# 1.8.1+cpu
# 0.9.1+cpu
 
# 2.进入powershell 用 nvidia-smi 查看 GPU支持的 CUDA Version: 11.2

# 3.根据CUDA版本到pytorch官网下载支持的GPU版本
    # conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c conda-forge

# 4.再次查看当前pytorch是否为支持GPU版本
print(torch.__version__)
print(torchvision.__version__)
# 输出为：
# 1.8.2+cu111
# 0.9.2+cu111
```

    1.8.2+cu111
    0.9.2+cu111
    1.8.2+cu111
    0.9.2+cu111
    


```python
# 查看gpu是否可用
is_gpu = torch.cuda.is_available()
print("查看gpu是否可用",is_gpu)

# 查看gpu数量
gpu_nums = torch.cuda.device_count()
print("查看gpu数量:",gpu_nums)

# 查看当前gpu号
gpu_index = torch.cuda.current_device()
print("查看当前gpu号:",gpu_index)

# 查看当前gpu号的设备名
device_name = torch.cuda.get_device_name(gpu_index)
print("查看当前gpu号的设备名:",device_name)


# 把tensor从内存复制到显存
x=torch.Tensor([1,2,3])
print("数据默认在RAM上:",x)
# 使用.cuda()可以将CPU上的Tensor转换（复制）到GPU上。
# 如果有多块GPU，用.cuda(i)来表示第 i 块GPU及相应的显存（i从0开始）且cuda(0)和cuda()等价。
x=x.cuda(gpu_index)
print("复制到显存上的数据:",x)



# 直接在显存上存储数据
device = torch.device('cuda')
x = torch.tensor([1, 2, 3], device=device)
# 或者
x = torch.tensor([1,2,3]).to(device)
print("直接在显存上存储的数据:",x)
print("直接在显存上存储的数据:",x.device)



# 对在GPU上的数据进行运算，那么结果还是存放在GPU上。
x = torch.tensor([1, 2, 3], device=torch.device('cuda'))
y = x**2
print("对在GPU上的数据进行运算,结果仍在GPU上",y)


# 存储在不同位置中的数据是不可以直接进行计算的。即存放在CPU上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行计算的。
x = torch.tensor([1, 2, 3], device=torch.device('cuda'))
y = torch.tensor([1, 2, 3], device=torch.device('cpu'))
# z = y + x
# print(z)
# RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!



# 有GPU就优先放在GPU上
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
x = torch.tensor([1, 2, 3], device=device)
x = torch.tensor([1,2,3]).to(device)


# 将模型放到GPU上
# net=net.cuda()
# print(type(net.parameters()))
# print(list(net.parameters())[0].device)
```

    查看gpu是否可用 True
    查看gpu数量: 1
    查看当前gpu号: 0
    查看当前gpu号的设备名: GeForce GTX 1050
    数据默认在RAM上: tensor([1., 2., 3.])
    复制到显存上的数据: tensor([1., 2., 3.], device='cuda:0')
    直接在显存上存储的数据: tensor([1, 2, 3], device='cuda:0')
    直接在显存上存储的数据: cuda:0
    对在GPU上的数据进行运算,结果仍在GPU上 tensor([1, 4, 9], device='cuda:0')
    cuda
    
