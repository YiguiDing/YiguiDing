---
title: 2.5.自动微分
layout: post
cover: /《动手学深度学习》(李沫)学习笔记/cover.jpg
---

# 2.5. 自动微分
* 深度学习框架通过自动计算导数，即自动微分（automatic differentiation）来加快求导。 
* 实际中，根据我们设计的模型，系统会构建一个计算图（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 
* 自动微分使系统能够随后反向传播梯度。
* 这里，反向传播（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

## 简单自动求梯度示例
* `x.requires_grad_(True)`使一个张量支持自动求梯度
* 再执行完`y=f(x)` 和 `y.backward()`后
* x.grad中将存储着y对x的梯度，也就是说x.grad中的元素grad_n 就是y对x_n的偏导
* 在默认情况下，PyTorch会累积梯度，所以在下一次计算梯度前，需要清除之前的值`x.grad.zero_()`


* 示例：$y=f(\mathbf{x})=2*\mathbf{x}^T\mathbf{x}$
* $\mathbf{x}.grad=\nabla_{\mathbf{x}} f(\mathbf{x}) $


```python
import torch
x = torch.arange(4.0)# x = torch.arange(4.0,requires_grad=True)
x.requires_grad_(True)#grad是梯度的意思，就是使其支持自动求梯度

y= 2 * torch.dot(x,x)

y.backward()#反向传播

print(x.grad)
print(x.grad==4*x)#因为y=2x^2所以∂y/∂x=4x 测试是否x.grad等于∂y/∂x=4x
```

    tensor([ 0.,  4.,  8., 12.])
    tensor([True, True, True, True])
    

## 非标量变量的反向传播之参数gradient
* backward()就是在根据计算图反向传播“梯度”，但是这个“梯度”只有在调用backward()的tensor是一个标量（scalar）时，才是真正意义上的梯度。
* 如果调用.backward()的tensor是一个标量，就可以省略gradient参数，而如果这个tensor是向量，则必须指明gradient参数，但这个参数对计算的作用是什么，官方文档中说的比较模糊。
* 实际上，如果是向量对向量求偏导，其实是先求出Jacobian(雅可比)矩阵中每一个元素的梯度值，然后将这个Jacobian(雅可比)矩阵与gradient参数对应的矩阵进行对应的点乘，才能最终得到梯度。

* 关于Jacobian(雅可比)矩阵
* 当`y`不是标量，而是一个向量时,此时yx都是向量,导数`dy/dx`是一个矩阵，这个矩阵就是Jacobian(雅可比)矩阵。
* 对于高阶和高维的`y`和`x`，求导的结果是一个高阶的张量。

* 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），
* 但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。
* 这里(**，我们的目的不是计算矩阵的微分，而是单独计算批量数据中每个样本的偏导数之和。**)

### 一个演示参数gradient在求梯度过程中起到作用的示例
* **设有x、y**
$$
x= \begin{bmatrix} x_1,x_2,x_3\end{bmatrix}^T = \begin{bmatrix} 0.0,2.0,8.0\end{bmatrix}^T \\
y= \begin{bmatrix} y_1,y_2,y_3\end{bmatrix}^T = \begin{bmatrix} 5.0,1.0,7.0\end{bmatrix}^T
$$

* **设有z=xy**
$$
z= xy =\begin{bmatrix} y_1x_1,y_2x_2,y_3x_3\end{bmatrix}^T
$$

* **则∂z/∂x的Jacobian(雅可比)矩阵为**
    * 雅可比矩阵，有时简称为雅可比矩阵，是一个一阶偏导矩阵(在某些情况下，术语“雅可比矩阵”也指雅可比矩阵的行列式)。 
    * **注意，在某些约定中，雅可比矩阵是上述矩阵的转置。**
    
$$
\frac{∂z}{∂x}
=
\begin{bmatrix} 
    \frac{\partial z}{\partial x_1} , \frac{\partial z}{\partial x_2} , \frac{\partial z}{\partial x_3} 
\end{bmatrix}^T
=
\begin{bmatrix}
    \begin{bmatrix}
        \frac{\partial z_1}{\partial x_1} \\
        \frac{\partial z_2}{\partial x_1} \\
        \frac{\partial z_3}{\partial x_1}  
    \end{bmatrix},
    \begin{bmatrix}
        \frac{\partial z_1}{\partial x_2} \\
        \frac{\partial z_2}{\partial x_2} \\
        \frac{\partial z_3}{\partial x_2}
    \end{bmatrix},
    \begin{bmatrix}
        \frac{\partial z_1}{\partial x_3} \\
        \frac{\partial z_2}{\partial x_3} \\
        \frac{\partial z_3}{\partial x_3}
    \end{bmatrix}
\end{bmatrix}^T
$$

$$
J=
(\frac{∂z}{∂x})^T
=
\begin{pmatrix}
    \begin{bmatrix}
        \begin{bmatrix}
            \frac{\partial z_1}{\partial x_1} \\
            \frac{\partial z_2}{\partial x_1} \\
            \frac{\partial z_3}{\partial x_1}  
        \end{bmatrix},
        \begin{bmatrix}
            \frac{\partial z_1}{\partial x_2} \\
            \frac{\partial z_2}{\partial x_2} \\
            \frac{\partial z_3}{\partial x_2}
        \end{bmatrix},
        \begin{bmatrix}
            \frac{\partial z_1}{\partial x_3} \\
            \frac{\partial z_2}{\partial x_3} \\
            \frac{\partial z_3}{\partial x_3}
        \end{bmatrix}
    \end{bmatrix}^T
\end{pmatrix}^T
=
\begin{bmatrix}
\frac{\partial z_1}{\partial x_1} , \frac{\partial z_1}{\partial x_2} , \frac{\partial z_1}{\partial x_3} \\
\frac{\partial z_2}{\partial x_1} , \frac{\partial z_2}{\partial x_2} , \frac{\partial z_2}{\partial x_3} \\
\frac{\partial z_3}{\partial x_1} , \frac{\partial z_3}{\partial x_2} , \frac{\partial z_3}{\partial x_3} 
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial z_1}{\partial x_1} , 0 , 0 \\
0 , \frac{\partial z_2}{\partial x_2} , 0 \\
0 , 0 , \frac{\partial z_3}{\partial x_3} 
\end{bmatrix}
=
\begin{bmatrix}
y_1 , 0 , 0 \\
0 , y_2 , 0 \\
0 , 0 , y_3 
\end{bmatrix}       
$$

* **设gradient为**
$$
G=
 \begin{bmatrix} 1.0 , 1.0 ,1.0 \end{bmatrix}^{T}
$$
* **将这个Jacobian(雅可比)矩阵与gradient参数对应的矩阵进行对应的点乘，得到最终的梯度grad**

$$
\mathbf{x}.grad
=
JG
=
\begin{bmatrix}
y_1 , 0 , 0\\ 0 , y_2 , 0\\ 0 , 0 , y_3
\end{bmatrix}
\begin{bmatrix}
1.0 \\
1.0 \\
1.0
\end{bmatrix}
=
\begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix}
=
\begin{bmatrix}
5.0 \\
1.0 \\
7.0 
\end{bmatrix}      
$$


```python
# 对非标量调用backward需要传入一个gradient参数
x=torch.tensor([0.0,2.0,8.0])
x.requires_grad_(True)

y=torch.tensor([5.0,1.0,7.0])
y.requires_grad_(True)

z = y * x

# print(z)

z.backward(gradient=torch.ones(len(x)))# 等价于z.sum().backward()吗？

x.grad

```




    tensor([5., 1., 7.])



## 分离计算

* 用于**将某些计算移动到记录的计算图之外**。
* 例如，假设$y=f_1(x)$而$z=f_2(y,x)$
* 如果想计算$z$关于$x$的梯度，但由于某种原因，我们希望将$y$视为一个常数，并且只考虑到$x$在$y$被计算后发挥的作用。
* 就需要分离$y$来返回一个新变量$u$，该变量与$y$具有相同的值，但丢弃计算图中如何计算$y$的任何信息。
* 换句话说，梯度不会向后流经$u$到$x$。
* 因此，下面的反向传播函数计算$z=f_2(y,x)$关于$x$的偏导数，同时将$y$作为常数处理，
* 而不是$z=f_2(y,x)=f_2(f_1(x),x)$关于$x$的偏导数。



```python
x=torch.arange(4.0)
x.requires_grad_(True)

y = x * x
u = y.detach()

z = u * x
z= z.sum() #使其变成一个标量

z.backward()

x.grad == u
```




    tensor([True, True, True, True])



* 由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，
* 得到`y=f(x)=x*x`关于`x`的导数，即`2*x`。



```python
x.grad.zero_()#清除原先的梯度

#y.sum().backward()
y=y.sum()#使其成为一个标量

y.backward()#反向传播

x.grad == 2 * x
```




    tensor([True, True, True, True])



## Python控制流的梯度计算

* 使用自动微分的一个好处是：
* [**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。
* 在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。



```python
def f(x):
    x_ = 2 * x
    while x_.norm() < 1000:
        x_ = x_ * 2

    if x_.sum() > 0:
        y = x_
    else:
        y = 100 * x_
    # 不管怎样y=kx,所以dy/dx=k
    return y
```

计算梯度。


```python
a = torch.randn(size=(), requires_grad=True)
y = f(a)
y.backward()
```

* 我们现在可以分析上面定义的`f`函数。
* 请注意，它在其输入`a`中是分段线性的。
* 换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`。
* 因此，我们可以用`d/a`验证梯度是否正确。


```python
a.grad == y / a
```




    tensor(True)



## 小结

* 深度学习框架可以自动计算导数：
    * 首先在要计算偏导数的变量上调用函数使其支持自动求梯度`x.requires_grad_(True)`
    * 然后我们对目标值计算`y=f(x)`
    * 执行它的反向传播函数`y.backward()`
    * 然后访问得到的梯度`x.grad`

## 练习

1. 为什么计算二阶导数比一阶导数的开销要更大？
    + 暂时不知道如何求二阶导数
1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。
    + 会报错
1. 在控制流的例子中，我们计算`y`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？
1. 重新设计一个求控制流梯度的例子，运行并分析结果。
    + 略
1. 使$f(x)=\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\cos(x)$。


### 第三题答题
* 在控制流的例子中，我们计算`y`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？
    * 会报错，因为当自变量为一个向量或矩阵时，因变量也将成为一个向量或矩阵，不再是一个标量了，不能直接调用y.backward()反向传播梯度
    * 可用
        + `y.sum().backward()` 或
        + `y.backward(gradient=torch.ones(size=a.shape))` 反向传播梯度



```python
def f(x):
    x_ = 2 * x
    while x_.norm() < 1000:
        x_ = x_ * 2

    if x_.sum() > 0:
        y = x_
    else:
        y = 100 * x_
    # 不管怎样y=f(x)=kx,所以dy/dx=k
    return y

a = torch.randn(size=[5,4], requires_grad=True)
print(a)

y = f(a)
# y.backward()
# y.sum().backward()
y.backward(gradient=torch.ones(size=a.shape))

a.grad == y / a
```

    tensor([[-0.5461,  0.2815,  1.2518, -0.3915],
            [-0.8608,  0.0377, -2.0479,  0.4740],
            [ 0.5945,  0.0711,  0.0782, -1.3386],
            [ 0.2825, -0.7495, -1.6161,  0.1722],
            [ 1.5139,  1.8324,  1.0184,  0.7167]], requires_grad=True)
    




    tensor([[True, True, True, True],
            [True, True, True, True],
            [True, True, True, True],
            [True, True, True, True],
            [True, True, True, True]])



### 第五题答题
* 使$f(x)=\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\cos(x)$。

#### 使能在jupytor中绘图的代码：


```python
#!pip install IPython
# from IPython import display

%matplotlib inline
# !pip install matplotlib
# import matplotlib

import matplotlib_inline
import matplotlib
import numpy

def use_svg_display():  #@save
    """使用svg格式在Jupyter中显示绘图"""
    #display.set_matplotlib_formats('svg')
    matplotlib_inline.backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):  #@save
    """设置matplotlib的图表大小"""
    use_svg_display()
    matplotlib.pyplot.rcParams['figure.figsize'] = figsize

#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """设置matplotlib的轴"""
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()

#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """绘制数据点"""
    if legend is None:
        legend = []

    set_figsize(figsize)
    # axes = axes if axes else d2l.plt.gca()
    axes = axes if axes else matplotlib.pyplot.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
```

#### 定义f(x)和f'(x)


```python
import torch
def f(x):#f(x)=\sin(x)
    
    return torch.sin(x)

def f_(x):#f'(x)=dy/dx
    x=x.clone().detach().requires_grad_(True)
    x.requires_grad_(True)

    y=f(x)
    y.backward(gradient=torch.ones(size=x.shape))

    return x.grad

```

#### 测试函数


```python
x = torch.arange(0, 3, 0.1)

print(f(x))
print(f_(x))
```

    tensor([0.0000, 0.0998, 0.1987, 0.2955, 0.3894, 0.4794, 0.5646, 0.6442, 0.7174,
            0.7833, 0.8415, 0.8912, 0.9320, 0.9636, 0.9854, 0.9975, 0.9996, 0.9917,
            0.9738, 0.9463, 0.9093, 0.8632, 0.8085, 0.7457, 0.6755, 0.5985, 0.5155,
            0.4274, 0.3350, 0.2392])
    tensor([ 1.0000,  0.9950,  0.9801,  0.9553,  0.9211,  0.8776,  0.8253,  0.7648,
             0.6967,  0.6216,  0.5403,  0.4536,  0.3624,  0.2675,  0.1700,  0.0707,
            -0.0292, -0.1288, -0.2272, -0.3233, -0.4161, -0.5048, -0.5885, -0.6663,
            -0.7374, -0.8011, -0.8569, -0.9041, -0.9422, -0.9710])
    

#### 绘图


```python
x = torch.arange(0, 2*3.1415, 0.1)

plot(x, [f(x), f_(x) ], 'x', 'f(x)', legend=['f(x)', "f'(x)"])
```


    
![svg](/动手学深度学习》(李沫)学习笔记/2.预备知识/output_28_0.svg)
    

